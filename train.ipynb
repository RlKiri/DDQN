{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先生成2048个时间矩阵\n",
    "class PFSPDataset(Dataset):\n",
    "    def __init__(self,num_machines,num_jobs,num_samples,random_seed=111):\n",
    "        super(PFSPDataset,self).__init__()\n",
    "        torch.manual_seed(random_seed)\n",
    "        self.data_set=[]\n",
    "        for l in tqdm(range(num_samples)):\n",
    "            x=np.random.randint(1,100,size=(num_machines,num_jobs))\n",
    "            x=torch.Tensor(x)\n",
    "            self.data_set.append(x)\n",
    "        self.size=len(self.data_set)\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data_set[idx]\n",
    "\n",
    "train_dataset = PFSPDataset(2,6,2048)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2048, \\\n",
    "           shuffle=True,pin_memory=True)\n",
    "\n",
    "\n",
    "\n",
    "def TrainModel(Agent,env,batch_size,episode,train_loader):\n",
    "    \n",
    "    \n",
    "    Reward_total = []\n",
    "    C_total = []\n",
    "    rewards_list = []\n",
    "    C = []\n",
    "    L=[]\n",
    "    \n",
    "    for i1 in range(episode):\n",
    "        \n",
    "        \n",
    "        \n",
    "        #函数1：从包含2048个矩阵中的data中选择128个的函数\n",
    "        #输入：包含2048个矩阵的dataloader\n",
    "        #输出：128个时间矩阵组成的列表\n",
    "        time_matrix128 = Agent.random_choice(train_loader,,2048,128)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        reward128 = []\n",
    "        action_chosen128 = []\n",
    "        next_state128 = []\n",
    "        for i2 in time_matrix128:\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #需要加一个环境重置的条件\n",
    "            state,done = env.reset()\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        \n",
    "            #开始对128个工件选顺序\n",
    "\n",
    "            \n",
    "            \n",
    "            #action_value = Agent.choose_action(state)\n",
    "            #函数2：choose_action函数\n",
    "            #输入：state\n",
    "            #输出：action的动作值\n",
    "            \n",
    "            #也就是eval_net的forward函数（也就是RLnetwork文件里面的CNN—FNN类）\n",
    "            #输入：state\n",
    "            #输出：各个action的动作值\n",
    "            \n",
    "            #需要加10%的随机选择这个功能吗\n",
    "            \n",
    "            action = torch.max(action_value, 1)[1].data.cpu().numpy()[0]\n",
    "            action_chosen.append(action)\n",
    "            \n",
    "            \n",
    "            #开始与环境互动：连续选择六次，得到下一个s和reward\n",
    "            state0 = []\n",
    "            reward0 = []\n",
    "            done_or_not0 = []\n",
    "\n",
    "            for i3 in range(6):\n",
    "                s0, r0, done0=env.step(action)\n",
    "                #Gantt(env.Machines)\n",
    "                state0.append(s0)\n",
    "                reward0.append(r0)\n",
    "                done_or_not0.append(done0)\n",
    "\n",
    "\n",
    "            reward = sum(reward0)\n",
    "            next_state = state0[5]\n",
    "            done = done_or_not0[5]\n",
    "                \n",
    "            #本来要存储memory的，现在不用了\n",
    "            #Agent.store_transition(state, action, reward, next_state)\n",
    "            #ep_reward += reward\n",
    "            \n",
    "            reward128.append(reward)\n",
    "            action_chosen128.append(action)\n",
    "            next_state128.append(next_state)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #当128个action，reward，next_state出来之后，到函数3\n",
    "        #函数3：learn函数（其中有PER_error函数)\n",
    "        #注：原来的learn函数是从memory中sample出来state,action，reward，next_state的\n",
    "        #改为\n",
    "        #输入：128个state,action，reward，next_state\n",
    "        #输出：各个action的动作值\n",
    "        \n",
    "        loss=Agent.learn()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #if i1 % 10 == 0：\n",
    "            #ret, f, C1, R1 = evaluate(i,Agent,env,loss)\n",
    "            #函数4：evaluate函数\n",
    "            #对模型进行评价\n",
    "            #没有讲到怎么evaluate我来提供一个思路\n",
    "            #再随机生成一批矩阵，用agent跑到最后，看看reward和loss\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        state = copy.copy(next_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
